\chapter{Conclusions and Future Work}\label{ch-conclusions}
This chapter concludes this thesis by including a summary of the contributions to the State of the Art (SOTA), as described in the previous chapters. These contributions address the research problems that are described in \hyperref[chap:3-objetives]{Chapter} \ref{chap:3-objetives}. The main focus of this thesis is studying if Explainable Artificial Intelligence (XAI) can be used for explaining the results of unsupervised learning algorithms for anomaly detection within two real-world contexts. This main hypothesis is divided into two sub-hypotheses. First, analysing that it is possible to indeed use XAI techniques over unsupervised learning algorithms for anomaly detection, and quantitatively measure the quality of explanations through XAI-specific metrics. Second, studying the applicability of XAI and anomaly unsupervised anomaly detection algorithms within real-world industry contexts, where we consider prior domain knowledge for both adjusting the explanations and for measuring their quality against it. 

Following this, \hyperref[sec-conclusions-contrib-1]{Subsection} \ref{sec-conclusions-contrib-1} delves into the contributions related to the first sub-hypothesis, while  \hyperref[sec-conclusions-contrib-2]{Subsection} \ref{sec-conclusions-contrib-2} does it for the second one. \hyperref[sec-conclusions-final]{Subsection} \ref{sec-conclusions-final} provides a final reflection about the potential impact of this work.

\section{Summary of the first contribution and future work}\label{sec-conclusions-contrib-1}
The first contribution of this thesis is related to a core study of the applicability of XAI for explaining unsupervised learning algorithms for anomaly detection, which also includes the usage of XAI-metrics for quantitatively measuring the quality of the explanations in order to compare the results of the XAI techniques among them. 

With that, we have addressed the problem of explaining these models by considering their output the same as the one from a binary classification model. However, some aspects that are different must be taken into account: there is commonly a great data imbalance between the two classes, there is normally a need to explain only one of the classes (outliers) in a counterfactual way, and the explanations must be P@1 within several use cases.

Because of that, even though any post-hoc XAI technique may be theoretically applied, the explanation results may differ, and some techniques may be more suitable than others for this context. In fact, we proposed \textbf{SVM+Prototypes reloaded} as an algorithm that could behave potentially better than others for the specific context of unsupervised anomaly detection. The algorithm serves for generating both post-hoc global and local counterfactual rule-based explanations that are model agnostic, and is a variant from a previous one used in the literature, and comes with two alternative methods for extracting the rules: \textbf{keep} as an approach that keeps all data points in every iteration for extracting the hypercubes, and \textbf{split} as an approach that splits the subspaces with a binary partition scheme until no points from the other class are inside the rules for the target class.

In order to quantitatively measure the results and compare the explanations outputs between the different XAI techniques, we need to use XAI-specific metrics. We used already existing XAI metrics
for measuring their \textit{comprehensibility} and \textit{representativenes} related aspects, and we have also proposed novel algorithms, \textbf{StabilityScore} and \textbf{DiversityScore}, for computing metrics related to the \textit{stability} and \textit{diversity} of the rule explanations. 

With that, we used both our XAI proposals, as well as other SOTA XAI model-agnostic techniques for rule extraction, and compared the explanation results generated over the decision of OneClass Support Vector Machine (OCSVM) models with different kernels and over different data sets. With that, we found out how XAI metrics indeed show that there are significant differences between the XAI techniques.

The results summarized in this subsection serve for checking the first sub-hypothesis, by both mathematically justifying XAI metrics, as well as evaluating them over different data sets in order to quantify explanation differences between rule extraction methods within the context of unsupervised anomaly detection.

All our contributions are included within a framework that standardizes the output of the different rule extraction techniques (by turning them into hypercubes) in order to carry out the evaluation through those metrics. This framework also prunes the rules, eliminating redundant ones. Our framework is available through an open source library.
\\
\\
Nonetheless, our contributions could be enhanced by carrying out additional studies. In the thesis, even though we used model-agnostic posthoc XAI techniques, we only assessed the results from OCSVM models for anomaly detection. Other unsupervised learning, such as IsolationForests \parencite{liu2008isolation}, Local Outlier Factors \parencite{breunig2000lof}, or Deep Learning based models, could be considered. This is also applicable to the XAI techniques themselves, since there are some alternatives, such as G-Rex algorithms \parencite{konig2008g}, not covered within the thesis. The research that lead to our proposed framework for generating and evaluating explanations for anomaly detection could also be continued with supervised ML models, since it is model-agnostic, needing only the input data and the output prediction. Thus, the usefulness of our rule-extraction algorithms and the novel XAI metrics that we proposed can also be studied over other types of ML algorithms and for other use cases beyond anomaly detection.

Within the thesis, we also proposed a simple metric for encapsulating all the individual metrics to simplify the comparisons and analyses. However, beyond computing the metric, we have not conducted an in-depth evaluation of it. Also, there is much room for improvement for finding an optimal function that weights appropriately every term.

%Along with that, rule extraction should also be designed to consider all types of comparisons ($\geq$, $\leq$, $>$ and $<$), and this is something that could also be considered in the cluster-based methods developed.
Along with that, rule extraction should also be designed to consider all types of comparisons ($\geq$, $\leq$, $>$ and $<$).

Finally, even though the metrics are theoretically useful from a quantitative point of view, the analysis should be complemented with user specific studies (considering different user profiles) in order to evaluate their usefulness and their possible alignment with prior domain knowledge.

\section{Summary of the second contribution and future work}\label{sec-conclusions-contrib-2}
The second contribution of this thesis is related to the usage of XAI techniques for explaining unsupervised anomaly detection algorithms within real-world industry contexts, through two use cases, one for communications data and one for the fuel consumption of petrol and diesel vehicles. Since these contexts have an already prior domain knowledge that can be related to the anomaly detection process, we studied both how this knowledge can be used for adjusting the explanations generated, as well as for assessing the quality of the fina explanations against it.

For the use case of communications data, we used a OCSVM for anomaly detection, and we have proposed an algorithm that generates visual and counterfactual explanations over it based on the information of the decision frontier. Our proposal generates visual explanations for a numerical feature with respect to every combination of categorical feature values using the information from the decision frontier of the Machine Learning (ML) algorithm. Along with this, we proposed the usage of a grid search algorithm based on MIES that includes prior domain knowledge, so the explanations generated are aligned with it. With that, the prior domain knowledge is used for choosing only hyperparameter configurations of the anomaly detection model that do not contradict that prior knowledge. We compared the results of this grid search algorithm variation against the results of a grid search that only aims to optimize the results from a pure model performance point of view, and we saw that the results do not have significant statistical differences, indicating that using prior knowledge does not penalize the results.

For the use case of fuel anomalies within vehicle fuel consumption, we have proposed a methodology for explainable unsupervised anomaly detection that detects, explains and provide fuel saving recommendations for those anomalies and for different user profiles, using also prior business domain knowledge. The explanations are feature relevance-based, obtained through surrogate Generalized Additive Models (GAM). This is the core of \textbf{RESYFEX}, a Recommender System (RecSys) that provides actionable recommendations for fuel saving considering two user profiles: fleet managers and fleet operators.

Within this last context, we initially used Explainable Boosting Machine (EBM) as the GAM algorithm, but we detected some limitations with it. First, the feature relevance explanations will be the same for all the vehicles within the fleet. Second, the relationship between feature values and feature relevance may not be monotonic when it should be. For addressing the first problem, we proposed a variation over EBM, \textbf{EBM\_var}, which adjusts the explanations based on the vehicle groups. For the second limitation, we considered two alternatives: using novel models that incorporate learning restrictions for providing monotonic relationships (using Constrained Generalized Additive 2 Model with Consideration of Higher-Order Interactions, CGA2M+, algorithm), or using an algorithm that removes some of the explanations, yielding only the ones that are monotonic. 

To compare the different surrogate model alternatives among themselves, we have performed evaluations regarding model performance (how well the model predicts the target feature), and XAI metrics, that compare the explanations generated in terms of representativeness, fidelity, stability, and contrastiveness.

Complementing this, we also carried out an analysis for ensuring that the explanations provided within this context are aligned to prior domain knowledge regarding the expected impact that those factors have on vehicle fuel consumption. Related to that, we also proposed metrics for measuring the consistency with apriori beliefs.

The results showed that the explanations are indeed aligned to prior domain knowledge regarding the factors that impact on vehicle fuel consumption. We have also shown how model performance is acceptable even after applying restrictions in the model for aligning the output to prior domain knowledge. Following this, we have also seen that the model performance is similar to that of unconstrained SOTA blackbox models. Within the context of XAI metrics, we saw that both CGA2M+ and EBM\_var provide similar or better results than EBM, while respectively solving monotonicity problems and considering the vehicle model and route type information for adjusting the explanations.

With this use case, we checked both sub-hypotheses, described in \hyperref[sec:Hypotheses]{Section} \ref{sec:Hypotheses}. First, we have applied an XAI approach for explaining the output of an unsupervised anomaly detection algorithm and we have compared the resulting explanations through XAI-specific metrics. Second, we have combined those XAI approaches with prior domain knowledge, showing that this does not penalize significantly the model performance, and we have analyzed how the final explanations are aligned to that prior knowledge. This last sub-hypothesis is also checked with the use case regarding communications data by showing how prior domain knowledge can be integrated within the XAI explanations, and this does not harm the predictive power of the model beneath them.
\\
\\
Regarding the future research lines, for the use case of vehicle fuel consumption, we see several areas where our current research can be continued. The first one is regarding the unsupervised algorithm for anomaly detection. Within our proposal, we have used a Box-Plot applied over the fuel consumption of the vehicles of a same group and route type since it directly provides a limit that helps seeing the threshold value that sets apart anomalous fuel consumption and non-anomalous one. It also provides a visual limit that highlights an additional insight for the users since they can see the average fuel split between inliers and outliers. However, there are other unsupervised algorithms that can be used if they can provide that threshold limit.

The second line is regarding the XAI metric usage. The literature proposes other aspects that can be measured in terms of human-friendly explanations, and it is important to both include those aspects, as well as assessing with different real users that the metrics do indeed measure that aspects.

For a third line, considering the use case of communications data, our proposal yields explanations for a numerical variable with respect to combinations of categorical variables. This could potentially lead to a scenario where there are many combinations, so the explanations are not easy to understand. Thus, the proposal could be improved by not showing all the combinations, and only focus on those where there are anomalies.

Fourth, the business domain knowledge is applied after generating the explanations, but it can also be considered before or during the training of the models. 

Following that, a final line of research related to both use cases is the study of alternatives for capturing the domain knowledge and combining it with the XAI techniques. In the thesis, we worked with rules and feature intervals for capturing the prior knowledge, but other approaches, such as ontologies, could provide more flexibility and yield better results. 


\section{Potential impact of this work}\label{sec-conclusions-final}

AI in general, and ML in particular, is having a greater impact in people's life, being more embedded in the day to day and in decision processes, both at the individual and at the business levels. Because of this increasing significant impact of ML, it is crucial that their development is done following a Responsible AI approach, where the goal is not just to optimize performance for a specific task, but to do so sustainably. Aspects such as explainability are linked to this, where the decisions made by a ML system are accompanied by an explanation of the reason for that decision, so that the human being who receives them can both learn from the system's reasoning and see if it can be trusted or not, depending on what reasons it provides. This is the area of XAI, which has many applications within the industrial field, as is the case of the explanations for unsupervised anomaly detection algorithms, on which the work of this thesis is focused.

With our work, we have contributed especially with a validation of XAI as techniques that are truly useful within the industrial field, since the question is not only that they can be used at a technical level on ML algorithms, but that the explanations that are generated are useful and easy to understand, and are consistent with prior knowledge of the domain. In this way, we have worked with XAI metrics to evaluate the explanations, both for analysing them by themselves, and in order to evaluate them based on prior knowledge, proposing some new metrics to measure other aspects quantitatively. In addition, we have proposed different ways to use XAI together with domain knowledge in industrial products for anomaly detection, where we have validated that XAI techniques serve to generate explanations that are really aligned with prior knowledge.

In this way, we believe that our work has served to bring closer the industrial field to the academic field within the XAI area, and in doing so, validate the utility of these XAI techniques for real-world industrial products. Looking to the future, this work seeks to serve as a reference to see how to apply XAI metrics to evaluate explanations, how to integrate prior knowledge to adjust them, and how, indeed, including explanations within an industrial product can enrich it while simultaneously serving as a way to validate the algorithms behind it with a more holistic approach.

\newpage